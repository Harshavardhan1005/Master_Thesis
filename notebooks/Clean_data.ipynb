{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean_data.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from shapely.geometry import Point\n",
        "from shapely.geometry.polygon import Polygon\n",
        "from ast import literal_eval\n",
        "import os\n",
        "import math\n",
        "tqdm.pandas(desc=\"Progress!\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "9Ej-Xh4c3C94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fms_data(fms_data_path):\n",
        "\n",
        "    df_fms = pd.read_csv(fms_data_path, sep=';', usecols=['Code', 'Name'], encoding='utf-8')\n",
        "    df_fms.rename(columns={'Code': 'key'}, inplace=True)\n",
        "    df_fms['key'] = df_fms['key'].astype(str)\n",
        "\n",
        "    return df_fms"
      ],
      "metadata": {
        "id": "3pvv34wa29ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_clean_zf_data(fms_data_path,zf_raw_data_path,zf_cleaned_data_path):\n",
        "\n",
        "    df_fms = load_fms_data(fms_data_path)\n",
        "\n",
        "    for file in glob(zf_raw_data_path + '/*'):\n",
        "        final_df = pd.DataFrame()\n",
        "        for folder in tqdm(np.sort(glob(file + '/*'))):\n",
        "            for path in np.sort(glob(folder + '/*')):\n",
        "\n",
        "                df_raw = pd.read_csv(path,compression='gzip',sep='|', usecols=['ts_msg_usec','timedelta_usec','key','value2','value'])\n",
        "                df_raw['Time_stamp'] = pd.to_datetime(df_raw['ts_msg_usec'] + df_raw['timedelta_usec'], unit='us')\n",
        "                df_raw['Time_stamp'] = df_raw['Time_stamp'].astype('datetime64[s]').dt.tz_localize('utc').dt.tz_convert(\n",
        "                    'Europe/Berlin')\n",
        "                df_raw.drop_duplicates(subset=['Time_stamp', 'key'], keep='last', inplace=True)\n",
        "\n",
        "                merge_df = df_raw.merge(df_fms, on='key', how='left')\n",
        "                merge_df.Name.fillna(merge_df.key, inplace=True)\n",
        "                merge_df.value2.fillna(merge_df.value, inplace=True)\n",
        "\n",
        "                df_raw = merge_df.pivot(index='Time_stamp', columns='Name', values='value2')\n",
        "                df_raw.reset_index(inplace=True)\n",
        "                df_raw['lat'] = df_raw['lat'].astype(float)\n",
        "                df_raw['lon'] = df_raw['lon'].astype(float)\n",
        "                df_raw.columns.name = None\n",
        "\n",
        "                df1 = pd.DataFrame()\n",
        "                df1['Time_stamp'] = pd.date_range(df_raw['Time_stamp'][0], df_raw['Time_stamp'][len(df_raw) - 1],\n",
        "                                                  freq='1s')\n",
        "                df1 = df1.merge(df_raw, on='Time_stamp', how='left')\n",
        "                df1['lat'] = df1['lat'].interpolate().ffill().bfill()\n",
        "                df1['lon'] = df1['lon'].interpolate().ffill().bfill()\n",
        "                df1['WheelBasedVehicleSpeed'] = df1['WheelBasedVehicleSpeed'].fillna(0)\n",
        "\n",
        "                final_df = pd.concat([final_df,df1])\n",
        "\n",
        "        final_df.to_csv(zf_cleaned_data_path + '/' + str(file[-7:]) + '.csv', sep=\",\", index=False)"
      ],
      "metadata": {
        "id": "mTopFaAX3SUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truck_position(df):\n",
        "\n",
        "    plant_1 = Polygon( [(9.489763, 47.660213), (9.491629, 47.661182), (9.492552, 47.660907), (9.494827, 47.660633),\n",
        "              (9.497251, 47.658797), (9.490556, 47.655126), (9.487123, 47.653854), (9.483626, 47.655834),\n",
        "              (9.485106, 47.657482), (9.48766, 47.659231), (9.489763, 47.660213)])\n",
        "    plant_2 = Polygon([(9.466138, 47.667208), (9.46352, 47.667251), (9.462512, 47.661471), (9.464314, 47.658335),\n",
        "              (9.473004, 47.658581), (9.473948, 47.662439), (9.471889, 47.664925), (9.466138, 47.667208)])\n",
        "\n",
        "\n",
        "    location = []\n",
        "    logic = []\n",
        "\n",
        "    flag = 1\n",
        "    value = ''\n",
        "  \n",
        "    for row in tqdm(df.to_dict('records')):\n",
        "        if plant_2.contains(Point(row['lon'], row['lat'])):\n",
        "            if flag == 0:\n",
        "                logic.append(1)\n",
        "            else:\n",
        "                logic.append(0)\n",
        "            location.append('2')\n",
        "            value = '2-road'\n",
        "            flag = 1\n",
        "\n",
        "        elif plant_1.contains(Point(row['lon'], row['lat'])):\n",
        "            if flag == 0:\n",
        "                logic.append(1)\n",
        "            else:\n",
        "                logic.append(0)\n",
        "            location.append('1')\n",
        "            value = '1-road'\n",
        "            flag = 1\n",
        "\n",
        "        else:\n",
        "            location.append(value)\n",
        "            if flag == 1:\n",
        "                logic.append(1)\n",
        "                flag = 0\n",
        "            else:\n",
        "                logic.append(0)\n",
        "\n",
        "    df['location'] = location\n",
        "    df['logic'] = logic\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "stDle_4B39Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def travel_time_less_3(new_df):\n",
        "    indexes = []\n",
        "    for index, row in tqdm(new_df.iterrows()):\n",
        "        if index < len(new_df) - 1:\n",
        "            if index % 2 != 0:\n",
        "                if (new_df.iloc[index, :]['Time_stamp'] - new_df.iloc[index - 1, :]['Time_stamp']) < pd.Timedelta(\n",
        "                        minutes=3):\n",
        "                    indexes.append(index)\n",
        "                    indexes.append(index - 1)\n",
        "    new_df.drop(new_df.index[indexes], inplace=True)\n",
        "    new_df.reset_index(drop=True, inplace=True)\n",
        "    return new_df"
      ],
      "metadata": {
        "id": "b0n5ncB35czj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def travel_time_information(new_df):\n",
        "\n",
        "    flag1 = 0\n",
        "    flag2 = 1\n",
        "    flag3 = 0\n",
        "    flag4 = 1\n",
        "    start1 = []\n",
        "    end1 = []\n",
        "    start2 = []\n",
        "    end2 = []\n",
        "    final_df1 = pd.DataFrame()\n",
        "    final_df2 = pd.DataFrame()\n",
        "\n",
        "    for index, row in tqdm(new_df.iterrows()):\n",
        "        if flag1 == 0:\n",
        "            if row['location'] == '2-road':\n",
        "                start2.append(row['Time_stamp'])\n",
        "                flag1 = 1\n",
        "                flag2 = 0\n",
        "            continue\n",
        "\n",
        "        if flag2 == 0:\n",
        "            if row['location'] == '1':\n",
        "                end1.append(row['Time_stamp'])\n",
        "                flag2 = 1\n",
        "                flag1 = 0\n",
        "            continue\n",
        "\n",
        "    for index, row in tqdm(new_df.iterrows()):\n",
        "        if flag3 == 0:\n",
        "            if row['location'] == '1-road':\n",
        "                start1.append(row['Time_stamp'])\n",
        "                flag3 = 1\n",
        "                flag4 = 0\n",
        "            continue\n",
        "\n",
        "        if flag4 == 0:\n",
        "            if row['location'] == '2':\n",
        "                end2.append(row['Time_stamp'])\n",
        "                flag4 = 1\n",
        "                flag3 = 0\n",
        "            continue\n",
        "    final_df1['start_plant1'] = start1\n",
        "    final_df1['end_plant2'] = end2\n",
        "\n",
        "    final_df2['start_plant2'] = start2\n",
        "    final_df2['end_plant1'] = end1\n",
        "\n",
        "    final_df1['travel_time(1-2)'] = final_df1['end_plant2'] - final_df1['start_plant1']\n",
        "    final_df2['travel_time(2-1)'] = final_df2['end_plant1'] - final_df2['start_plant2']\n",
        "\n",
        "    final_df1['travel_time(1-2)'] = final_df1['travel_time(1-2)'].apply(lambda x: x.total_seconds() / 60)\n",
        "    final_df2['travel_time(2-1)'] = final_df2['travel_time(2-1)'].apply(lambda x: x.total_seconds() / 60)\n",
        "\n",
        "    return final_df1,final_df2"
      ],
      "metadata": {
        "id": "ifnoZSu54SWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_gps_and_speed_infromation(final_df1,final_df2,df,speed_threshold):\n",
        "\n",
        "    final_df2['GPS_2_1_lat'] = final_df2.progress_apply(\n",
        "        lambda x: df[(df['Time_stamp'].between(x['start_plant2'], x['end_plant1']))]['lat'].values, axis=1)\n",
        "    final_df2['GPS_2_1_lon'] = final_df2.progress_apply(\n",
        "        lambda x: df[(df['Time_stamp'].between(x['start_plant2'], x['end_plant1']))]['lon'].values, axis=1)\n",
        "    final_df1['GPS_1_2_lat'] = final_df1.progress_apply(\n",
        "        lambda x: df[(df['Time_stamp'].between(x['start_plant1'], x['end_plant2']))]['lat'].values, axis=1)\n",
        "    final_df1['GPS_1_2_lon'] = final_df1.progress_apply(\n",
        "        lambda x: df[(df['Time_stamp'].between(x['start_plant1'], x['end_plant2']))]['lon'].values, axis=1)\n",
        "    \n",
        "\n",
        "    final_df2['speed_2_1'] = final_df2.progress_apply(\n",
        "        lambda x: df[(df['Time_stamp'].between(x['start_plant2'], x['end_plant1']))][\n",
        "            'WheelBasedVehicleSpeed'].values,\n",
        "        axis=1)\n",
        "    final_df1['speed_1_2'] = final_df1.progress_apply(\n",
        "        lambda x: df[(df['Time_stamp'].between(x['start_plant1'], x['end_plant2']))][\n",
        "            'WheelBasedVehicleSpeed'].values,\n",
        "        axis=1)\n",
        "    final_df1['speed_threshold'] = final_df1['speed_1_2'].progress_apply(\n",
        "        lambda x: statistics.mean([1 if float(i) < speed_threshold else 0 for i in x]))\n",
        "    final_df2['speed_threshold'] = final_df2['speed_2_1'].progress_apply(\n",
        "        lambda x: statistics.mean([1 if float(i) < speed_threshold else 0 for i in x]))\n",
        "    \n",
        "    return final_df1,final_df2"
      ],
      "metadata": {
        "id": "4_W-sZr34VFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_route_information(final_df1,final_df2,name,zf_preprocessed_data1_path,zf_preprocessed_data2_path):\n",
        "\n",
        "    routes_2_1 = []\n",
        "    routes_1_2 = []\n",
        "    \n",
        "    route_1= Polygon([(9.484763, 47.658797), (9.481587, 47.660994), (9.478283, 47.663277), (9.482145, 47.664867),\n",
        "              (9.487467, 47.661168), (9.484763, 47.658797)])\n",
        "    route_2= Polygon([(9.474764, 47.658277), (9.4806, 47.658971), (9.481115, 47.657815), (9.475107, 47.656832),\n",
        "              (9.474764, 47.658277)])\n",
        "    route_3= Polygon([(9.485664, 47.665792), (9.490042, 47.668624), (9.496651, 47.665503), (9.497509, 47.661977),\n",
        "              (9.494247, 47.660763), (9.485664, 47.665792)])\n",
        "    route_4= Polygon([(9.475193, 47.669665), (9.476137, 47.665908), (9.48266, 47.666312), (9.482231, 47.669896),\n",
        "              (9.475193, 47.669665)])\n",
        "\n",
        "    for index, row in tqdm(final_df2.iterrows()):\n",
        "        for i in range(len(final_df2['GPS_2_1_lat'][index])):\n",
        "            if route_4.contains(Point(final_df2['GPS_2_1_lon'][index][i], final_df2['GPS_2_1_lat'][index][i])):\n",
        "                routes_2_1.append(4)\n",
        "                break\n",
        "            elif route_3.contains(Point(final_df2['GPS_2_1_lon'][index][i], final_df2['GPS_2_1_lat'][index][i])):\n",
        "                routes_2_1.append(3)\n",
        "                break\n",
        "            elif route_2.contains(Point(final_df2['GPS_2_1_lon'][index][i], final_df2['GPS_2_1_lat'][index][i])):\n",
        "                routes_2_1.append(2)\n",
        "                break\n",
        "            elif route_1.contains(Point(final_df2['GPS_2_1_lon'][index][i], final_df2['GPS_2_1_lat'][index][i])):\n",
        "                routes_2_1.append(1)\n",
        "                break\n",
        "\n",
        "    for index, row in tqdm(final_df1.iterrows()):\n",
        "        for i in range(len(final_df1['GPS_1_2_lat'][index])):\n",
        "            if route_4.contains(Point(final_df1['GPS_1_2_lon'][index][i], final_df1['GPS_1_2_lat'][index][i])):\n",
        "                routes_1_2.append(4)\n",
        "                break\n",
        "            elif route_3.contains(Point(final_df1['GPS_1_2_lon'][index][i], final_df1['GPS_1_2_lat'][index][i])):\n",
        "                routes_1_2.append(3)\n",
        "                break\n",
        "            elif route_2.contains(Point(final_df1['GPS_1_2_lon'][index][i], final_df1['GPS_1_2_lat'][index][i])):\n",
        "                routes_1_2.append(2)\n",
        "                break\n",
        "            elif route_1.contains(Point(final_df1['GPS_1_2_lon'][index][i], final_df1['GPS_1_2_lat'][index][i])):\n",
        "                routes_1_2.append(1)\n",
        "                break\n",
        "\n",
        "    final_df2['route_2_1'] = routes_2_1\n",
        "    final_df1['route_1_2'] = routes_1_2\n",
        "\n",
        "    final_df1 = final_df1[['start_plant1','end_plant2','travel_time(1-2)','route_1_2','speed_threshold']]\n",
        "    final_df2 = final_df2[['start_plant2','end_plant1','travel_time(2-1)','route_2_1','speed_threshold']]\n",
        "\n",
        "    final_df1.to_csv(zf_preprocessed_data1_path + '/' + str(name) + '(1-2).csv', sep=\",\", index=False)\n",
        "    final_df2.to_csv(zf_preprocessed_data2_path + '/' + str(name) + '(2-1).csv', sep=\",\", index=False)\n"
      ],
      "metadata": {
        "id": "qWESpXSV4y36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_zf_data(zf_cleaned_data_path,zf_preprocessed_data1_path,zf_preprocessed_data2_path,speed_threshold):\n",
        "\n",
        "    for path in np.sort(glob(zf_cleaned_data_path + '/*')):\n",
        "\n",
        "        df = pd.read_csv(path,sep=\",\", usecols=['Time_stamp','lat','lon','WheelBasedVehicleSpeed'],encoding='utf-8')\n",
        "        df['Time_stamp'] = pd.to_datetime(df['Time_stamp'], infer_datetime_format=True)\n",
        "\n",
        "        print('-->Finding the position of the truck')\n",
        "        df = truck_position(df)\n",
        "        new_df = df[df['logic'] == 1]\n",
        "        new_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        print('-->Removing records having travel time less that 3 minutes')\n",
        "        new_df = travel_time_less_3(new_df)\n",
        "\n",
        "        print('-->Finding the Travel time information')\n",
        "        final_df1,final_df2 = travel_time_information(new_df)\n",
        "\n",
        "        print('-->Fetching Gps inforamtion and speed information')\n",
        "        final_df1,final_df2 = fetch_gps_and_speed_infromation(final_df1,final_df2,df,speed_threshold)\n",
        "\n",
        "        print('-->route information')\n",
        "        name = path[-11:-4]\n",
        "        fetch_route_information(final_df1,final_df2,name,zf_preprocessed_data1_path,zf_preprocessed_data2_path)"
      ],
      "metadata": {
        "id": "6r4WhuKI45FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_zf_data(zf_dataset_csv,zf_preprocessed_data2_path):\n",
        "\n",
        "    df_zf = pd.DataFrame()\n",
        "    for path in np.sort(glob(zf_preprocessed_data2_path+'/*')):\n",
        "        df = pd.read_csv(path)\n",
        "        df_zf = pd.concat([df,df_zf])\n",
        "\n",
        "    \n",
        "    df_zf = df_zf[df_zf['route_2_1'] == 1]\n",
        "    df_zf = df_zf[df_zf['travel_time(2-1)'] < 20]\n",
        "\n",
        "    df_zf['start_plant2'] = pd.to_datetime(df_zf['start_plant2'],infer_datetime_format=True)\n",
        "    df_zf['Week_Day'] = df_zf['start_plant2'].dt.weekday\n",
        "    df_zf['Week_Day_Name'] = df_zf['start_plant2'].dt.day_name()\n",
        "    df_zf['Week'] = df_zf['start_plant2'].dt.isocalendar().week\n",
        "    df_zf['time'] = df_zf['start_plant2'].dt.time\n",
        "    df_zf['Hour'] = df_zf['time'].apply(lambda x: x.hour)\n",
        "    df_zf['Minutes'] = df_zf['time'].apply(lambda x: x.minute)\n",
        "    df_zf['Seconds'] = df_zf['time'].apply(lambda x: x.second)\n",
        "    print(df_zf.head())\n",
        "    df_zf.to_csv(zf_dataset_csv,index=False)"
      ],
      "metadata": {
        "id": "56YB7QbV75gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_and_split_data(zf_dataset_csv,weather_data_path,test_data_path,train_data_path,split_ratio):\n",
        "\n",
        "    df_zf = pd.read_csv(zf_dataset_csv)\n",
        "    df_weather = pd.read_csv(weather_data_path)\n",
        "\n",
        "    df_weather['Timestamp'] = pd.to_datetime(df_weather['Timestamp'])\n",
        "    df_zf['start_plant2'] = pd.to_datetime(df_zf['start_plant2'])\n",
        "\n",
        "    df_weather['time'] = df_weather['Timestamp'].apply(lambda x: x.strftime(\"%Y-%m-%d %H\"))\n",
        "    df_zf['time'] = df_zf['start_plant2'].apply(lambda x: x.strftime(\"%Y-%m-%d %H\"))\n",
        "\n",
        "    merge_df = pd.merge(df_zf,df_weather,on='time')\n",
        "\n",
        "    merge_df = merge_df[['Week_Day','Week','Hour','Minutes','Seconds','speed_threshold','Clouds','Temp','Wind_deg','Wind_speed','Rain_1h','Rain_3h','Snow_1h','Snow_3h','travel_time(2-1)']]\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        merge_df,\n",
        "        test_size=split_ratio,\n",
        "        random_state=42\n",
        "    )\n",
        "    train.to_csv(train_data_path, sep=\",\", index=False, encoding=\"utf-8\")\n",
        "    test.to_csv(test_data_path, sep=\",\", index=False, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "uC4aNiceJP8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fms_data_path = '/content/drive/MyDrive/Data/zf_data/FMS_signals_raw.csv'\n",
        "\n",
        "zf_raw_data_path = '/content/drive/MyDrive/Data/zf_data/raw_data'\n",
        "zf_cleaned_data_path = '/content/drive/MyDrive/Data/zf_data/cleaned_data'\n",
        "zf_preprocessed_data1_path = '/content/drive/MyDrive/Data/zf_data/preprocessed_data/plant1-plant2'\n",
        "zf_preprocessed_data2_path = '/content/drive/MyDrive/Data/zf_data/preprocessed_data/plant2-plant1'\n",
        "zf_dataset_csv = '/content/drive/MyDrive/Data/zf_data/merge_data/zf_data.csv'\n",
        "\n",
        "test_data_path = '/content/drive/MyDrive/Data/merged_data/test.csv'\n",
        "train_data_path = '/content/drive/MyDrive/Data/merged_data/train.csv'\n",
        "weather_data_path = '/content/drive/MyDrive/Data/weather_data/weather.csv'\n",
        "\n",
        "split_ratio = 0.2\n",
        "speed_threshold = 5\n",
        "\n",
        "print('Load and Clean ZF Data:')\n",
        "load_and_clean_zf_data(fms_data_path,zf_raw_data_path,zf_cleaned_data_path)\n",
        "\n",
        "print('Preprocess ZF Data:')\n",
        "preprocess_zf_data(zf_cleaned_data_path,zf_preprocessed_data1_path,zf_preprocessed_data2_path,speed_threshold)\n",
        "\n",
        "print('Merge ZF Data:')\n",
        "merge_zf_data(zf_dataset_csv,zf_preprocessed_data2_path)\n",
        "\n",
        "print('Merge and Split Complete Data:')\n",
        "merge_and_split_data(zf_dataset_csv,weather_data_path,test_data_path,train_data_path,split_ratio)"
      ],
      "metadata": {
        "id": "cXtEe4mC6ayX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_weather_data(start,end):\n",
        "\n",
        "    df_weather = pd.DataFrame()\n",
        "\n",
        "    for i in tqdm(range(math.ceil((end-start)/(167*3600)))):\n",
        "        df = pd.read_json('http://history.openweathermap.org/data/2.5/history/city?q=Friedrichshafen,DE&type=hour&start={0}&cnt=168&appid=212117db1236e6aee483f90d1592f01b'.format(start))\n",
        "        df_weather = pd.concat([df_weather,df])\n",
        "        start = start + (167 * 3600)\n",
        "\n",
        "    df_weather.to_csv(weather_raw_data_path,index=False)"
      ],
      "metadata": {
        "id": "QKaoVDZkeByJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_weather_data(weather_cleaned_data_path,weather_raw_data_path):\n",
        "\n",
        "    # Wed Sep 01 2021 00:00:01 GMT+0200 (Central European Summer Time)\n",
        "    # Tue Mar 01 2022 00:00:01 GMT+0100 (Central European Standard Time)\n",
        "    # store_weather_data(1630447201,1646089201)\n",
        "\n",
        "    Timestamp = []\n",
        "    Clouds = []\n",
        "    Temp = []\n",
        "    Weather = []\n",
        "    Wind_deg = []\n",
        "    Wind_speed = []\n",
        "    Rain_1h = []\n",
        "    Rain_3h = []\n",
        "    Snow_1h = []\n",
        "    Snow_3h = []\n",
        "\n",
        "    df_weather = pd.read_csv(weather_raw_data_path)\n",
        "    df_weather['list'] = df_weather['list'].apply(lambda x: literal_eval(x))\n",
        "    final_weather = pd.DataFrame(\n",
        "        columns=['Timestamp', 'Clouds', 'Temp', 'Weather', 'Wind_deg', 'Wind_speed', 'Rain_1h', 'Rain_3h', 'Snow_1h',\n",
        "                 'Snow_3h'])\n",
        "\n",
        "    for row in df_weather.to_dict('records'):\n",
        "        Timestamp.append(pd.to_datetime(row['list']['dt'], unit='s').tz_localize('utc').tz_convert('Europe/Berlin'))\n",
        "        Clouds.append(row['list']['clouds']['all'])\n",
        "        Temp.append(row['list']['main']['temp'])\n",
        "        Weather.append(row['list']['weather'][0]['description'])\n",
        "\n",
        "        if 'rain' in row['list'].keys():\n",
        "            if '1h' in row['list']['rain'].keys():\n",
        "                Rain_1h.append(row['list']['rain']['1h'])\n",
        "            else:\n",
        "                Rain_1h.append(np.NaN)\n",
        "            if '3h' in row['list']['rain'].keys():\n",
        "                Rain_3h.append(row['list']['rain']['3h'])\n",
        "            else:\n",
        "                Rain_3h.append(np.NaN)\n",
        "        else:\n",
        "            Rain_1h.append(np.NaN)\n",
        "            Rain_3h.append(np.NaN)\n",
        "\n",
        "        if 'snow' in row['list'].keys():\n",
        "            if '1h' in row['list']['snow'].keys():\n",
        "                Snow_1h.append(row['list']['snow']['1h'])\n",
        "            else:\n",
        "                Snow_1h.append(np.NaN)\n",
        "            if '3h' in row['list']['snow'].keys():\n",
        "                Snow_3h.append(row['list']['snow']['3h'])\n",
        "            else:\n",
        "                Snow_3h.append(np.NaN)\n",
        "        else:\n",
        "            Snow_1h.append(np.NaN)\n",
        "            Snow_3h.append(np.NaN)\n",
        "\n",
        "        if 'wind' in row['list'].keys():\n",
        "            Wind_deg.append(row['list']['wind']['deg'])\n",
        "            Wind_speed.append(row['list']['wind']['speed'])\n",
        "        else:\n",
        "            Wind_deg.append(np.NaN)\n",
        "            Wind_speed.append(np.NaN)\n",
        "\n",
        "    final_weather['Timestamp'] = Timestamp\n",
        "    final_weather['Clouds'] = Clouds\n",
        "    final_weather['Temp'] = Temp\n",
        "    final_weather['Weather'] = Weather\n",
        "    final_weather['Rain_1h'] = Rain_1h\n",
        "    final_weather['Rain_3h'] = Rain_3h\n",
        "    final_weather['Snow_1h'] = Snow_1h\n",
        "    final_weather['Snow_3h'] = Snow_3h\n",
        "    final_weather['Wind_deg'] = Wind_deg\n",
        "    final_weather['Wind_speed'] = Wind_speed\n",
        "\n",
        "    final_weather.to_csv(weather_cleaned_data_path,index=False)\n"
      ],
      "metadata": {
        "id": "qHAH59u0ettE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_weather_data(weather_cleaned_data_path,weather_raw_data_path,weather_dataset_csv):\n",
        "    #clean_weather_data(weather_cleaned_data_path,weather_raw_data_path)\n",
        "    df = pd.read_csv(weather_cleaned_data_path)\n",
        "    df.fillna(0,inplace=True)\n",
        "    df.to_csv(weather_dataset_csv,index=False)"
      ],
      "metadata": {
        "id": "azZvCkXHfCio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_raw_data_path = '/content/drive/MyDrive/Data/weather_data/raw_data/weather.csv'\n",
        "weather_cleaned_data_path = '/content/drive/MyDrive/Data/weather_data/cleaned_data/cleaned_weather.csv'\n",
        "weather_dataset_csv = '/content/drive/MyDrive/Data/weather_data/weather.csv'\n",
        "preprocess_weather_data(weather_cleaned_data_path,weather_raw_data_path,weather_dataset_csv)"
      ],
      "metadata": {
        "id": "t0VJjEzheOYH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}