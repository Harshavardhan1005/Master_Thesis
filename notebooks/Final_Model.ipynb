{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Model.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM, Dropout\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator"
      ],
      "metadata": {
        "id": "2h12UKkvU71w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "id": "j_6M9KYVwsZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.__version__"
      ],
      "metadata": {
        "id": "8aWLhBXVwiJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow"
      ],
      "metadata": {
        "id": "wF98tai9xl0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_zf = pd.read_csv('/content/drive/MyDrive/Thesis/check/zf_data.csv')\n",
        "df_weather = pd.read_csv('/content/drive/MyDrive/Thesis/check/weather.csv')"
      ],
      "metadata": {
        "id": "hQVkNciaVR3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_zf"
      ],
      "metadata": {
        "id": "qFBovKciWsbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather"
      ],
      "metadata": {
        "id": "cb2t74XqWu-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather['Timestamp'] = pd.to_datetime(df_weather['Timestamp'])\n",
        "df_zf['start_plant2'] = pd.to_datetime(df_zf['start_plant2'])\n",
        "\n",
        "df_weather['time'] = df_weather['Timestamp'].apply(lambda x: x.strftime(\"%Y-%m-%d %H\"))\n",
        "df_zf['time'] = df_zf['start_plant2'].apply(lambda x: x.strftime(\"%Y-%m-%d %H\"))\n",
        "\n",
        "merge_df = pd.merge(df_zf,df_weather,on='time')\n",
        "\n",
        "merge_df = merge_df[['Week_Day','Week','Hour','Minutes','Seconds','speed_threshold','Clouds','Temp','Wind_deg','Wind_speed','Rain_1h','Rain_3h','Snow_1h','Snow_3h','travel_time(2-1)']] "
      ],
      "metadata": {
        "id": "XSUWfwZrW39S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_df"
      ],
      "metadata": {
        "id": "0-5mno1qXzMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = merge_df.iloc[0:-150]\n",
        "test = merge_df.iloc[-150:]"
      ],
      "metadata": {
        "id": "J-2sQct0X2TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = train['travel_time(2-1)']\n",
        "test_y = test['travel_time(2-1)']\n",
        "\n",
        "train_x = train.drop('travel_time(2-1)', axis=1)\n",
        "test_x = test.drop('travel_time(2-1)', axis=1)"
      ],
      "metadata": {
        "id": "RU3nFRyiYDMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_percentage_error(test,predictions): \n",
        "    test = np.array(test)\n",
        "    predictions = np.array(predictions)\n",
        "    return np.mean(np.abs((test - predictions) / test)) * 100"
      ],
      "metadata": {
        "id": "Am1C3YjRYNts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_metrics(actual, pred):\n",
        "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
        "    mae = mean_absolute_error(actual, pred)\n",
        "    r2 = r2_score(actual, pred)\n",
        "    return rmse, mae, r2"
      ],
      "metadata": {
        "id": "KxKazlveYbyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr1 = RandomForestRegressor()\n",
        "lr1.fit(train_x, train_y)\n",
        "\n",
        "predicted_qualities = lr1.predict(test_x)\n",
        "\n",
        "(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
        "\n",
        "print(\"Random Forest model\" )\n",
        "print(\"  RMSE: %s\" % rmse)\n",
        "print(\"  MAE: %s\" % mae)\n",
        "print(\"  R2: %s\" % r2)\n",
        "\n",
        "print(mean_absolute_percentage_error(test_y,predicted_qualities))"
      ],
      "metadata": {
        "id": "ihR5tbF9YgYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr1.get_params()"
      ],
      "metadata": {
        "id": "9WGC2I_YOyJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "n_estimators = [10,20,30,40,5060,70,80,90,100]\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [7,8,9,10,11,12]\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [2]"
      ],
      "metadata": {
        "id": "bIP9ef66Ibc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_test_accuracy = pd.DataFrame(columns = ['n_estimators','max_depth','min_samples_split','min_samples_leaf','RMSE','MAE', 'MAPE'])\n",
        "for x in tqdm(list(itertools.product(n_estimators, max_depth,min_samples_split,min_samples_leaf)),desc='Random Forest Hyperparameter Tunning'):\n",
        "  rf = RandomForestRegressor(n_estimators = x[0],max_depth = x[1], min_samples_split = x[2],min_samples_leaf=x[3],n_jobs=-1)\n",
        "  rf.fit(train_x, train_y)\n",
        "\n",
        "  predicted_qualities = rf.predict(test_x)\n",
        "\n",
        "  (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
        "\n",
        "  mape = mean_absolute_percentage_error(test_y,predicted_qualities)\n",
        "  rf_test_accuracy_one = pd.DataFrame(index = range(1),columns = ['n_estimators','max_depth','min_samples_split','min_samples_leaf','RMSE','MAE','MAPE']) \n",
        "\n",
        "  rf_test_accuracy_one.loc[:,'n_estimators'] = x[0]\n",
        "  rf_test_accuracy_one.loc[:,'max_depth'] = x[1]\n",
        "  rf_test_accuracy_one.loc[:,'min_samples_split'] = x[2]\n",
        "  rf_test_accuracy_one.loc[:,'min_samples_leaf'] = x[3]\n",
        "  rf_test_accuracy_one.loc[:,'RMSE'] = rmse\n",
        "  rf_test_accuracy_one.loc[:,'MAE'] = mae\n",
        "  rf_test_accuracy_one.loc[:,'MAPE'] = mape\n",
        "\n",
        "  rf_test_accuracy = pd.concat([rf_test_accuracy,rf_test_accuracy_one])\n",
        "  "
      ],
      "metadata": {
        "id": "1e-AI7lxIJpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_test_accuracy[rf_test_accuracy['MAPE'] == rf_test_accuracy['MAPE'].min()]"
      ],
      "metadata": {
        "id": "YjpVuVX6LC3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_y.values, label='Actual Values')\n",
        "plt.plot(predicted_qualities,color='red',label = 'Forecasting Prediction')\n",
        "plt.legend(loc='best')"
      ],
      "metadata": {
        "id": "-YNqYG7cB_46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr2 = xgboost.XGBRegressor(verbosity=0)\n",
        "lr2.fit(train_x, train_y)\n",
        "\n",
        "predicted_qualities = lr2.predict(test_x)\n",
        "\n",
        "(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
        "\n",
        "print(\"XGBoost model\" )\n",
        "print(\"  RMSE: %s\" % rmse)\n",
        "print(\"  MAE: %s\" % mae)\n",
        "print(\"  R2: %s\" % r2)\n",
        "print(mean_absolute_percentage_error(test_y,predicted_qualities))"
      ],
      "metadata": {
        "id": "_BMSn3EvYk1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XG Boost\n",
        "# Number of trees\n",
        "n_estimators = \n",
        "# Maximum number of levels in tree\n",
        "max_depth = [12,13,14,15]\n",
        "#minimum sum of weights of all observations required in a child\n",
        "min_child_weight = [1,2]\n",
        "#Gamma specifies the minimum loss reduction required to make a split\n",
        "gamma = [1,5]\n",
        "# boosting learning rate\n",
        " = [.1,.05,.01]"
      ],
      "metadata": {
        "id": "uNa_kB8IRdnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_test_accuracy = pd.DataFrame(columns = ['n_estimators','max_depth','min_child_weight','gamma','learning_rate','RMSE','MAE', 'MAPE'])\n",
        "for x in tqdm(list(itertools.product(n_estimators, max_depth,min_child_weight,gamma,learning_rate)),desc='Random Forest Hyperparameter Tunning'):\n",
        "  xgb = xgboost.XGBRegressor(n_estimators = x[0],max_depth = x[1], min_child_weight = x[2],gamma=x[3],learning_rate=x[4],verbosity = 0,n_jobs=-1)\n",
        "  xgb.fit(train_x, train_y)\n",
        "\n",
        "  predicted_qualities = xgb.predict(test_x)\n",
        "\n",
        "  (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
        "\n",
        "  mape = mean_absolute_percentage_error(test_y,predicted_qualities)\n",
        "  xgb_test_accuracy_one = pd.DataFrame(index = range(1),columns = ['n_estimators','max_depth','min_child_weight','gamma','learning_rate','RMSE','MAE', 'MAPE']) \n",
        "\n",
        "  xgb_test_accuracy_one.loc[:,'n_estimators'] = x[0]\n",
        "  xgb_test_accuracy_one.loc[:,'max_depth'] = x[1]\n",
        "  xgb_test_accuracy_one.loc[:,'min_child_weight'] = x[2]\n",
        "  xgb_test_accuracy_one.loc[:,'gamma'] = x[3]\n",
        "  xgb_test_accuracy_one.loc[:,'learning_rate'] = x[4]\n",
        "  xgb_test_accuracy_one.loc[:,'RMSE'] = rmse\n",
        "  xgb_test_accuracy_one.loc[:,'MAE'] = mae\n",
        "  xgb_test_accuracy_one.loc[:,'MAPE'] = mape\n",
        "\n",
        "  xgb_test_accuracy = pd.concat([xgb_test_accuracy,xgb_test_accuracy_one])\n",
        "  "
      ],
      "metadata": {
        "id": "PgzVUdEIRdqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_test_accuracy[xgb_test_accuracy['MAPE'] == xgb_test_accuracy['MAPE'].min()]"
      ],
      "metadata": {
        "id": "6wtUgurNTUZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_y.values, label='Actual Values')\n",
        "plt.plot(predicted_qualities,color='red',label = 'Forecasting Prediction')\n",
        "plt.legend(loc='best')"
      ],
      "metadata": {
        "id": "nU69wkYHB50g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vo9IUjOcHHYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the data \n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_x)\n",
        "scaled_train = scaler.transform(train_x)\n",
        "scaled_test = scaler.transform(test_x)"
      ],
      "metadata": {
        "id": "A3NAiDhYe93F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train generator\n",
        "n_input = 12\n",
        "n_feature = 14\n",
        "\n",
        "train_generator = TimeseriesGenerator(scaled_train,train_y.values,length=n_input, batch_size=1)"
      ],
      "metadata": {
        "id": "GSk-2iPAfVzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_input, n_feature)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "Rby5_7TLfV2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_generator,epochs= 50)"
      ],
      "metadata": {
        "id": "o-X2zO8DfV5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Loss\n",
        "loss= model.history.history['loss']\n",
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "BI2abojtV30T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data predictions\n",
        "test_predictions = []\n",
        "\n",
        "# last n_input points from the training set\n",
        "first_eval_batch = scaled_train[-n_input:]\n",
        "# reshape this to the format of RNN (same format as TimeseriesGeneration)\n",
        "current_batch = first_eval_batch.reshape((1,n_input,n_feature))\n",
        "\n",
        "for i in range(len(test_x)):\n",
        "    \n",
        "    # One timestep ahead of historical 12 points\n",
        "    current_pred = model.predict(current_batch)[0]\n",
        "    #store that prediction\n",
        "    test_predictions.append(current_pred)\n",
        "    \n",
        "    # update the current batch to include prediction\n",
        "    current_batch = np.append(current_batch[:,1:,:],[[scaled_test[i]]], axis= 1)"
      ],
      "metadata": {
        "id": "tZdFBQsCV33J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = test_predictions"
      ],
      "metadata": {
        "id": "ZJfMw4nObP3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_y.values, label='Actual Values')\n",
        "plt.plot(pred,color='red',label = 'Forecasting Prediction')\n",
        "plt.legend(loc='best')"
      ],
      "metadata": {
        "id": "I-7TsUUmdFEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(rmse, mae, r2) = eval_metrics(test_y, pred)\n",
        "\n",
        "print(\"XGBoost model\" )\n",
        "print(\"  RMSE: %s\" % rmse)\n",
        "print(\"  MAE: %s\" % mae)\n",
        "print(\"  R2: %s\" % r2)"
      ],
      "metadata": {
        "id": "REdQrXhPaiVg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}